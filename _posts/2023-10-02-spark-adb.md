---
layout: post
title: Pipelines with Spark Databricks and Report
date: 2023-10-01 17:34 +0000
last_modified_at: 2023-10-01 17:34 +0000
author : Luciano Nieto
tags: [spark, databricks, report, powerbi]
toc:  true
---

This pipeline provides a comprehensive demonstration of the end-to-end process, excluding the initial data source integration. It encompasses best practices in data modeling (star schema), leverages the power of Spark for distributed computation, and ultimately delivers a compelling outcome in the form of a detailed Power BI report. 

## Technical solution

![](/imgs/sp9.png)

## Requirements

To use these repo, you will need the following:

- Clone this repository to your local machine and upload the INTEGRATION_ECOMMERCE.ipynb.
- Download the assets from Kaggle and upload into the Databricks.
- Databricks: File -> "Upload data to DBFS"...


## Notebook

This is the notebook managed in Databricks:
> 
![](/imgs/sp1.png)
![](/imgs/sp2.png)
![](/imgs/sp3.png)
![](/imgs/sp4.png)
![](/imgs/sp5.png)
![](/imgs/sp6.png)
![](/imgs/sp10.png)
![](/imgs/sp11.png)


## Modeling

The model in Power BI was developed with Star Schema approach.

![](/imgs/.png)

## Report

- URL **[here](https://app.powerbi.com/groups/me/reports/61263d98-76c5-4906-a1b5-533711cb66ca?ctid=82535b8f-842b-47aa-9f25-da369e36c49f&pbi_source=linkShare)**

The link becomes accessible only during the active trial period.

![](/imgs/sp8.png)


## More

- GitHub Repo **[here](https://github.com/lucnietoX/spark_pipelines.git)**.
